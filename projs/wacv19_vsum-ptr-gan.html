<!DOCTYPE HTML>

<html>

<head>
  <title>[WACV'19] Attentive and Adversarial Learning for Video Summarization</title>

  <link rel="stylesheet" href="template.css">
  <link rel="icon" href="../assests/icon.png">
</head>

<body>
  <br>

  <div class="home">
    <center>
      <a href="https://tsujuifu.github.io"><img src="home.png" width=60px /></a>
    </center>
  </div>

  <center><span style="font-size: 44px; font-weight: bold;">Attentive and Adversarial Learning for Video Summarization</span></center><br/>
  <table align=center width=750px>
    <tr>
      <td align=center width=150px>
        <center>
          <span style="font-size: 22px">
            <a href="https://tsujuifu.github.io/" target="_blank">Tsu-Jui Fu</a>
          </span>
        </center>
      </td>

      <td align=center width=150px>
        <center>
          <span style="font-size: 22px">
            Shao-Heng Tai
          </span>
        </center>
      </td>

      <td align=center width=150px>
        <center>
          <span style="font-size: 22px">
            <a href="http://www.cs.nthu.edu.tw/~htchen/" target="_blank">Hwann-Tzong Chen</a>
          </span>
        </center>
      </td>
    </tr>
  </table>

  <table align=center width=600px style="padding-top: 0px; padding-bottom: 20px;">
    <tr>
      <td align=center width=600px>
        <center>
          <span style="font-size: 22px">National Tsing Hua University, Hsinchu</span>
        </center>
      </td>
    </tr>
  </table>

  <table align=center width=1000px>
    <tr>
      <td align=center width=650px>
        <center>
          <span style="font-size: 22px">IEEE Winter Conference on Applications of Computer Vision (<b>WACV</b>) 2019 (<b>oral</b>)</span>
        </center>
      </td>
    </tr>
  </table>

  <table align=center width=600px>
    <tr>
      <td align=center width=150px>
        <center>
          <span style="font-size: 22px">
            <a href="../pubs/wacv19_vsum-ptr-gan.pdf" target="_blank">[Paper]</a>
          </span>
        </center>
      </td>

      <td align=center width=150px>
        <center>
          <span style="font-size: 22px">
            <a href="https://github.com/tsujuifu/pytorch_vsum-ptr-gan" target="_blank">[Code]</a>
          </span>
        </center>
      </td>
    </tr>
  </table>

  <br>

  <table align=center width=300px>
    <tr>
      <td aligh=center width=645px>
        <iframe src="https://www.youtube.com/embed/0irqOrpAYgw" width=645px height=367px allowfullscreen></iframe>
      </td>
    </tr>
  </table>

  <br>

  <center>
    <h1>Abstract</h1>
  </center>
  <div style="width: 750px; margin: 0 auto; text-align=center; text-align: justify; text-justify: inter-ideograph;">
    This paper aims to address the video summarization problem via <b>attention-aware and adversarial training</b>. We formulate the problem as a <b>sequence-to-sequence</b> task, where the input sequence is an original video and the output sequence is its summarization. We propose a <b>GAN-based</b> training framework, which combines the merits of unsupervised and supervised video summarization approaches. The <b>generator is an attention-aware Ptr-Net</b> that generates the cutting points of summarization fragments. The <b>discriminator is a 3D CNN classifier</b> to judge whether a fragment is from a ground-truth or a generated summarization. The experiments show that our method achieves state-of-the-art results on SumMe, TVSum, YouTube, and LoL datasets. Our Ptr-Net generator can overcome the <b>unbalanced training-test length</b> in the seq2seq problem, and our discriminator is effective in <b>leveraging unpaired summarizations</b> to achieve better performance.
  </div>

  <br><hr>

  <center>
    <h1>Overview</h1>
  </center>
  <table align=center width=800px>
    <center><img src = "wacv19_vsum-ptr-gan/overview.jpg" width=800px /></center>
  </table>
  <br>
  <div style="width: 750px; margin: 0 auto; text-align=center; text-align: justify; text-justify: inter-ideograph;">
    The proposed method for video summarization is a <b>GAN model</b> that contains a <b>Ptr-Net as the generator</b> and a <b>3D CNN binary classifier as the discriminator</b>. The generator summarizes the input video into several fragments, and the discriminator distinguishes whether a fragment is from ground-truth summarization or is generated by the summarizer. The goal is to make the generated summarization fragments as authentic to the discriminator as possible.
  </div>

  <br><hr>

  <center>
    <h1>Ptr-Net Generator</h1>
  </center>
  <table align=center width=800px>
    <center><img src = "wacv19_vsum-ptr-gan/ptr-gen.png" width=800px /></center>
  </table>
  <br>
  <div style="width: 750px; margin: 0 auto; text-align=center; text-align: justify; text-justify: inter-ideograph;">
    We formulate video summarization as a seq2seq problem. The input sequence is an original video and the <b>output sequence comprises the summarization fragments</b>. Inspired by the question answering task, we create a Ptr-Net generator that uses <b>bi-directional LSTM as the encoder</b> and an <b>attention-based cutting point predictor as the decoder</b>. The decoder does not produce consecutive frames as the output but <b>only tuples of the starting and ending points of fragments</b>. This way, we only need to maintain very compact information about the fragments. The Ptr-Net generator would <b>not suffer from the difficulty of producing longer output sequences</b> and would be easier to train.
  </div>

  <br><hr>

  <center>
    <h1>Discriminator</h1>
  </center>
  <table align=center width=800px>
    <center><img src = "wacv19_vsum-ptr-gan/3d-dis.png" width=800px /></center>
  </table>
  <br>
  <div style="width: 750px; margin: 0 auto; text-align=center; text-align: justify; text-justify: inter-ideograph;">
    Instead of using RNNs again, we use a <b>CNN architecture to construct the discriminators</b>. CNNs are good at extracting dominant receptive fields, and we thus consider that it should be sufficient for a CNN to tell the authenticity simply based on several frames of a fragment. We <b>randomly pick a fixed number of frames chronologically in a fragment</b>, and concatenate the frames into a 3D cuboid as the discriminator input. Then, the cuboid goes through the 3D CNN and the output is a prediction on the cuboid being from the groundtruth summarization (True) or from the generator (False). Since the outputs of the ptr-generator are <b>discrete probability</b> distribution and <b>are not differentiable</b>, we cannot update our generator from the discriminator via simple backpropagation. Here, like SeqGAN, we adopt <b>policy gradient to estimate the approximate gradient</b>. That is, we consider the output from the discriminator as the reward and <b>use REINFORCE algorithm</b> to train our generator so that it can maximize the reward from our discriminator.
  </div>

  <br><hr>

  <center>
    <h1>Learning from Unpaired</h1>
  </center>
  <div style="width: 750px; margin: 0 auto; text-align=center; text-align: justify; text-justify: inter-ideograph;">
    Although the original videos are not available to form training pairs, we may <b>use only the summarization videos</b> to train the discriminator as well as the generator under our GAN framework. For those unpaired summarization videos, since we do not know the exact split position for each fragment, we <b>apply naive frame clustering to split whole video summarization into several fragments</b>. We can <b>use these unpaired summarization fragments as the ground-truth samples</b> to train our discriminator.
  </div>

  <br><hr>

  <center>
    <h1>Experimental Result</h1>
  </center>
  <table align=center width=800px>
    <center><img src = "wacv19_vsum-ptr-gan/exp-1.png" width=600px /></center>
  </table>
  <br>
  <table align=center width=800px>
    <center><img src = "wacv19_vsum-ptr-gan/exp-2.png" width=600px /></center>
  </table>
  <br>
  <table align=center width=800px>
    <center><img src = "wacv19_vsum-ptr-gan/exp-3.png" width=600px /></center>
  </table>
  <br>
  <table align=center width=800px>
    <center><img src = "wacv19_vsum-ptr-gan/exp-4.png" width=600px /></center>
  </table>
  <br>
  <table align=center width=800px>
    <center><img src = "wacv19_vsum-ptr-gan/exp-5.png" width=600px /></center>
  </table>
  <br>
  <table align=center width=800px>
    <center><img src = "wacv19_vsum-ptr-gan/exp-6.jpg" width=600px /></center>
  </table>

  <br><hr>

  <center>
    <h1>Citation</h1>
  </center>
  <div style="width: 750px; margin: 0 auto; text-align: justify; text-justify: inter-ideograph;">
    @inproceedings{fu2019vsum-ptr-gan, <br>
      &emsp; author = {Tsu-Jui Fu and Shao-Heng Tai and Hwann-Tzong Chen}, <br>
      &emsp; title = {Attentive and Adversarial Learning for Video Summarization}, <br>
      &emsp; booktitle = {IEEE Winter Conference on Applications of Computer Vision (WACV)}, <br>
      &emsp; year = {2019} <br>
    }
  </div>

  <br>

  <center>
    template from <a href="https://pathak22.github.io/zeroshot-imitation/" target="_blank">pathak22</a>
  </center>

  <br>

</body>

</html>
